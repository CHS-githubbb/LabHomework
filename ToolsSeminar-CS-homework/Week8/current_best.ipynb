{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN\n",
    "\n",
    "In this assignment, you need to build a simple CNN to classify images in CIFAR-10 dataset.\n",
    "\n",
    "![cifar10](https://pytorch.org/tutorials/_images/cifar10.png)\n",
    "\n",
    "The framework is given below. What you need to do is to build your net. Then tune the hyperparameters, train the NN, and achieve **60%+** accuracy. (about 1h training on CPU is enough)\n",
    "\n",
    "You can also refer to the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hyperparameters. They are maybe not the best, you need to tune, especially the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 32 # Mini-batch size\n",
    "DEVICE = torch.device('cpu')\n",
    "CONFIG = {\"lr\": LEARNING_RATE,\n",
    "          \"num_epochs\": NUM_EPOCHS,\n",
    "          \"batch_size\": BATCH_SIZE,\n",
    "          \"network\": \"LeNet feat. Adam\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Define your NN here.\n",
    "\n",
    "Some reference networks:\n",
    "* LeNet\n",
    "* AlexNet\n",
    "* VGG\n",
    "* GoogleLeNet (maybe to large for your PC to train)\n",
    "* ResNet (too large)\n",
    "\n",
    "Ref: [Overview of CNN Development - Zhihu](https://zhuanlan.zhihu.com/p/66215918)\n",
    "\n",
    "Do not directly copy others' code. Check these networks' structure and write code yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (FC): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Implementation of a simple Convolutional neural network.\n",
    "    HINT: You can refer to several famous CNNs, like LeNet5, VGG16, ResNet\n",
    "        Actually you cannot directly copy the code in the demo,\n",
    "        since you need to recalculate the shape of the tensor.\n",
    "    \"\"\"\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(  #3*32*32\n",
    "            nn.Conv2d(3, 6, 5),         #6*28*28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),         #6*14*14\n",
    "            nn.Conv2d(6, 16, 5),        #16*10*10\n",
    "            nn.MaxPool2d(2, 2),         #16*5*5\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.FC = nn.Sequential(  #3*32*32\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "        \n",
    "        #self.C5 = nn.Linear(16*5*5, 120)\n",
    "        #self.F6 = nn.Linear(120, 84),\n",
    "        #self.Output = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        #x = nn.ReLU(self.C5(x))\n",
    "        #x = nn.ReLU(self.F6(x))\n",
    "        #x = self.Output(x)\n",
    "        x = self.FC(x)\n",
    "        return x\n",
    "    \"\"\"END OF YOUR CODE\"\"\"\n",
    "\n",
    "model = Net().to(device=DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR-10 dataset. Downloading will be started automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # magic numbers\n",
    "                         std=[0.229, 0.224, 0.225]) # 3 channels\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer. You can **choose other** optimizers as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_eval, loader_eval, criterion_eval):\n",
    "    model_eval.eval()\n",
    "    loss_eval = 0\n",
    "    correct = 0.\n",
    "    pbar = tqdm(total = len(loader_eval), desc='Evaluation', ncols=100)\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader_eval:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            loss_eval += criterion_eval(output, target).item()\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    loss_eval = loss_eval / loader_eval.dataset.__len__()\n",
    "    accuracy = correct / loader_eval.dataset.__len__()\n",
    "    response = {'loss': loss_eval, 'acc': accuracy}\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training function defined below. Be careful of [overfitting](https://en.wikipedia.org/wiki/Overfitting)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - Epoch 0: 100%|██████████████████████████████████████████| 1563/1563 [01:15<00:00, 20.67it/s]\n",
      "Evaluation: 100%|███████████████████████████████████████████████| 1563/1563 [00:49<00:00, 31.49it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*- Epoch 0 -*-*-*-*-*-\n",
      "Train Loss: 0.042686\t\n",
      "Train Acc: 0.503480\t\n",
      "Eval Loss: 0.043640\t\n",
      "Eval Acc: 0.496200\t\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Downloads\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "Train - Epoch 1: 100%|██████████████████████████████████████████| 1563/1563 [01:13<00:00, 21.18it/s]\n",
      "Evaluation: 100%|███████████████████████████████████████████████| 1563/1563 [00:52<00:00, 30.04it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.61it/s]\n",
      "Train - Epoch 2:   0%|                                             | 2/1563 [00:00<01:27, 17.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*- Epoch 1 -*-*-*-*-*-\n",
      "Train Loss: 0.035422\t\n",
      "Train Acc: 0.593960\t\n",
      "Eval Loss: 0.037461\t\n",
      "Eval Acc: 0.571100\t\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - Epoch 2: 100%|██████████████████████████████████████████| 1563/1563 [01:15<00:00, 20.57it/s]\n",
      "Evaluation: 100%|███████████████████████████████████████████████| 1563/1563 [00:49<00:00, 31.66it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████| 313/313 [00:09<00:00, 31.40it/s]\n",
      "Train - Epoch 3:   0%|                                             | 2/1563 [00:00<01:20, 19.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*- Epoch 2 -*-*-*-*-*-\n",
      "Train Loss: 0.033910\t\n",
      "Train Acc: 0.611280\t\n",
      "Eval Loss: 0.036612\t\n",
      "Eval Acc: 0.580100\t\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - Epoch 3: 100%|██████████████████████████████████████████| 1563/1563 [01:13<00:00, 21.23it/s]\n",
      "Evaluation: 100%|███████████████████████████████████████████████| 1563/1563 [00:49<00:00, 31.82it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.08it/s]\n",
      "Train - Epoch 4:   0%|                                             | 2/1563 [00:00<01:34, 16.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*- Epoch 3 -*-*-*-*-*-\n",
      "Train Loss: 0.032246\t\n",
      "Train Acc: 0.630240\t\n",
      "Eval Loss: 0.036375\t\n",
      "Eval Acc: 0.586600\t\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - Epoch 4: 100%|██████████████████████████████████████████| 1563/1563 [01:17<00:00, 20.09it/s]\n",
      "Evaluation: 100%|███████████████████████████████████████████████| 1563/1563 [00:49<00:00, 31.58it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████| 313/313 [00:09<00:00, 31.87it/s]\n",
      "Train - Epoch 5:   0%|                                             | 2/1563 [00:00<01:23, 18.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*- Epoch 4 -*-*-*-*-*-\n",
      "Train Loss: 0.027999\t\n",
      "Train Acc: 0.681180\t\n",
      "Eval Loss: 0.033664\t\n",
      "Eval Acc: 0.621800\t\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - Epoch 5: 100%|██████████████████████████████████████████| 1563/1563 [01:16<00:00, 20.50it/s]\n",
      "Evaluation: 100%|███████████████████████████████████████████████| 1563/1563 [00:50<00:00, 31.19it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████| 313/313 [00:10<00:00, 31.01it/s]\n",
      "Train - Epoch 6:   0%|                                             | 2/1563 [00:00<01:26, 17.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*- Epoch 5 -*-*-*-*-*-\n",
      "Train Loss: 0.027405\t\n",
      "Train Acc: 0.690860\t\n",
      "Eval Loss: 0.033991\t\n",
      "Eval Acc: 0.621800\t\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - Epoch 6: 100%|██████████████████████████████████████████| 1563/1563 [01:21<00:00, 19.20it/s]\n",
      "Evaluation: 100%|███████████████████████████████████████████████| 1563/1563 [00:52<00:00, 29.77it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████| 313/313 [00:09<00:00, 31.76it/s]\n",
      "Train - Epoch 7:   0%|                                             | 2/1563 [00:00<01:18, 19.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*- Epoch 6 -*-*-*-*-*-\n",
      "Train Loss: 0.026853\t\n",
      "Train Acc: 0.695220\t\n",
      "Eval Loss: 0.034305\t\n",
      "Eval Acc: 0.621900\t\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train - Epoch 7:  52%|██████████████████████▎                    | 812/1563 [00:40<00:33, 22.18it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e391d8b29f64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc = np.zeros(NUM_EPOCHS)\n",
    "eval_acc = np.zeros(NUM_EPOCHS)\n",
    "train_loss = np.zeros(NUM_EPOCHS)\n",
    "eval_loss = np.zeros(NUM_EPOCHS)\n",
    "\n",
    "model.train()\n",
    "for epoch_idx in range(NUM_EPOCHS):\n",
    "    pbar = tqdm(total = len(train_dataloader), desc='Train - Epoch {}'.format(epoch_idx), ncols=100)\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    train_resp = evaluate(model, train_dataloader, criterion)\n",
    "    eval_resp = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "    print ('-*-*-*-*-*- Epoch {} -*-*-*-*-*-'.format(epoch_idx))\n",
    "    print ('Train Loss: {:.6f}\\t'.format(train_resp['loss']))\n",
    "    print ('Train Acc: {:.6f}\\t'.format(train_resp['acc']))\n",
    "    print ('Eval Loss: {:.6f}\\t'.format(eval_resp['loss']))\n",
    "    print ('Eval Acc: {:.6f}\\t'.format(eval_resp['acc']))\n",
    "    print ('\\n')\n",
    "\n",
    "    train_acc[epoch_idx] = train_resp['acc']\n",
    "    eval_acc[epoch_idx] = eval_resp['acc']\n",
    "    train_loss[epoch_idx] = train_resp['loss']\n",
    "    eval_loss[epoch_idx] = eval_resp['loss']\n",
    "\n",
    "    # save model and training data\n",
    "    torch.save(model, 'simple-cnn.pth'.format(CONFIG[\"network\"]))\n",
    "    np.savez('simple-cnn',config=CONFIG,train_acc=train_acc, eval_acc=eval_acc,train_loss=train_loss, eval_loss=eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc = np.zeros(NUM_EPOCHS)\n",
    "eval_acc = np.zeros(NUM_EPOCHS)\n",
    "train_loss = np.zeros(NUM_EPOCHS)\n",
    "eval_loss = np.zeros(NUM_EPOCHS)\n",
    "\n",
    "def plot_acc(train_acc,eval_acc):\n",
    "    plt.plot(train_acc,label=\"Train\")\n",
    "    plt.plot(eval_acc,label=\"Test\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(train_loss,eval_loss):\n",
    "    plt.plot(train_loss,label=\"Train\")\n",
    "    plt.plot(eval_loss,label=\"Test\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_acc(train_acc,eval_acc)\n",
    "plot_loss(train_loss,eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
